log_dir: ${oc.env:ROOT,.}/logs

training:
  max_round: 500
  max_stage: 1
  num_generations: 3
  seed: 561

game_manager:
  _target_: genrl.game.game_manager.BaseGameManager
  max_stage: ${training.max_stage}
  max_round: ${training.max_round}
  game_state: 
    _target_: genrl.state.GameState
    round: 0
    stage: 0
  reward_manager:
    _target_: genrl.rewards.DefaultRewardManager
    reward_fn_store:
      _target_: genrl.rewards.reward_store.RewardFnStore
      max_rounds: ${training.max_round}
      reward_fn_stores:
        - _target_: genrl.rewards.reward_store.RoundRewardFnStore
          num_stages: ${training.max_stage}
          reward_fns:
            - _target_: genrl.examples.rgym.rewards.RGRewards
  trainer:
    _target_: genrl.trainer.grpo_trainer.GRPOLanguageTrainerModule
    models:
      - _target_: transformers.AutoModelForCausalLM.from_pretrained
        pretrained_model_name_or_path: Gensyn/Qwen2.5-0.5B-Instruct
        config: trl.trainer.GRPOConfig
    epsilon: 0.2
    epsilon_high: 0.28
    beta: 0.0
    num_generations: ${training.num_generations}
    log_with: tensorboard
    log_dir: ${log_dir}
  data_manager:
    _target_: genrl.examples.rgym.data.ReasoningGymDataManager
    yaml_config_path: "src/genrl/examples/rgym/datasets.yaml"
    num_train_samples: 2
    num_evaluation_samples: 0
    system_prompt_id: 'default'
    seed: ${training.seed}
    num_transplant_trees: 5
  run_mode: "train"